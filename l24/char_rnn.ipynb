{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks at character-level\n",
    "Recurrent Neural Networks (RNNs) are popular models in many NLP tasks such as\n",
    "\n",
    "* image captioning\n",
    "* sentiment analysis\n",
    "* machine translation\n",
    "* video classification\n",
    "\n",
    "But what is RNNs? In this notebook, we are going to learn about RNNs by implementing/training a vanilla RNNs to generate text character by character, the notebook covers the following topics\n",
    "\n",
    "* Brief introduction to RNN and some great resources on RNNs for interested readers\n",
    "* Detail model architecture with brief discussion on pros/cons of vanilla RNNs\n",
    "* Implement vanilla RNNs with Numpy\n",
    "* Implement vanilla RNNs with TensorFlow\n",
    "$\n",
    "\\newcommand{\\real}{\\mathbb{R}}\n",
    "\\newcommand{\\targets}{\\mathrm{targets}}\n",
    "\\newcommand{\\argmax}[1]{\\mathrm{arg}\\max_{#1}}\n",
    "\\newcommand{\\mvec}[1]{\\pmb{#1}}\n",
    "$\n",
    "\n",
    "# Introduction to RNNs\n",
    "Recall ourselves that we have been looking at [Vanilla Neural Networks](#) and [Convolutional Networks](#), they both share one limitation: \n",
    "\n",
    "<center>they accept a fixed-size vector (an image) as input and produce a fixed-size vector as output (scores of different classes). </center>\n",
    "\n",
    "Why is it limited? It's because in many problems, our inputs/outputs will have variable length e.g\n",
    "\n",
    "* in text translation the input can be a word or a sequence of words\n",
    "* in image captioning the output can be a word or a paragraph\n",
    "\n",
    "This's where RNNs comes in, it allows us to operate over sequences of input and generate a sequences of output. In this notebook we look at the vanilla RNNs which has following diagram\n",
    "\n",
    "![alt text](./vanilla_rnn_diagram.png \"Vanilla RNN diagram\")\n",
    "\n",
    "where \n",
    "\n",
    "* $h_0\\in \\real^H$ is initial hidden state with \n",
    "* $x_1,\\ldots,x_n\\in \\real^D$ is a sequence of input \n",
    "* $h_1,\\ldots,h_n\\in \\real^H$ is a sequence of hidden states\n",
    "* $y_1,\\ldots,y_n\\in \\real^C$ is a sequence of output\n",
    "\n",
    "The vanilla RNNs has following dynamics\n",
    "$$\n",
    "\\left\\{\\begin{array}{rl}\n",
    "h_t &= \\tanh\\left(W_{xh} \\times x_{t} + W_{hh}\\times h_{t-1} + b_h\\right)\\\\\n",
    "y_t &= W_{hy} \\times h_t + b_y\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "## Model architecture for char-RNNs\n",
    "In this notebook, we want to use character-wise RNNs, let's look at how we can choose $H, D, C$\n",
    "\n",
    "* $H$ is a hyper-parameter to control hidden-size so we choose whatever makes RNNs works best (Note that in image-captioning sometime if we can take $h_0$ as output of NNs/CNNs for example *fc7* of AlexNet then $H$ has same size as output of *fc7*)\n",
    "* $D$ is input size, and we use one-hot encoding for character so $D=$ number of characters e.g if our characters is only English lower case (26 characters) then we have $D=26$ and\n",
    "$$\n",
    "\\text{\"a\"} \\mapsto \\left(\\begin{array}{c}1\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\\right)_{26\\times 1}\n",
    "$$\n",
    "Normally, we will load all the text-data, then get the unique list of our characters and then set $D$ is the size of that unique list (see the implementation for more detail)\n",
    "* $C$ is output size, in our example we have $C=D$ since the output is also characters. For other application e.g image captioning, $C$ will be the size of vocabulary while $D$ is the size of image input.\n",
    "* $n$ is the length of input sequence, in this notebook we use $n=25$.\n",
    "\n",
    "To understand better the forward pass of RNNs, we look at example from [Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "<img src=\"./charseq.jpeg\" alt=\"hello example\" style=\"width: 500px;\"/>\n",
    "\n",
    "In above example, the RNNs takes input sequences \"h\", \"e\", \"l\", \"l\" and it wants to compute scores output sequence, the target sequence is \"e\", \"l\", \"l\", \"o\". \n",
    "\n",
    "More generally, we will feed the RNN a huge chunk of text and ask it to train $W_{xh}, W_{hh}, b_h, W_{hy}, b_{y}$ so it can model the probability distribution of the next character given a sequence of previous characters i.e\n",
    "* given input-sequence $x_1,\\ldots,x_n$ \n",
    "* and target-sequence $\\targets_1,\\ldots\\targets_n$\n",
    "* RNNs forward pass gives us $y_1,\\ldots,y_n$ which allows us to models the distribution $p_t$ of output sequence  using following form\n",
    "$$\n",
    "p_t[c] = p(\\targets_t = c) = \\frac{\\exp(y_t[c])}{\\sum_{c=1}^{26}\\exp(y_t[c])}\n",
    "$$\n",
    "$p_t$ is the probability distribution over character at $t-th$ position of output sequence. Note here, given a trained model, we can use $p_t$ to predicts the character at time $t$ as such \n",
    "$$c_t = \\argmax{c}p_t[c]$$\n",
    "\n",
    "In order to train $W_{xh}, W_{hh}, b_h, W_{hy}, b_{y}$, we need to define an objective function (cost function), and here we use the **Softmax** loss function (also commonly referred to as the cross-entropy loss)\n",
    "$$\n",
    "L_t(y_t, \\targets_t) = - \\log\\left(p_t[\\targets_t]\\right)\n",
    "$$\n",
    "Intuitively, by minimizing loss function, it's equivalent to maximize the chance $p_t[\\targets_t]$ close to 1.0 (in other word it predicts correct next character).\n",
    "\n",
    "## RNNs resources\n",
    "We have done a brief introduction on RNNs (quite a long one), for anyone interested in knowing more about RNNs and LSTMs please check out few great blog posts\n",
    "1. [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), by Andrej Karpathy\n",
    "2. [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), by Christopher Olah\n",
    "\n",
    "Let's do some implementation on character-wise RNNs\n",
    "\n",
    "# Implementing character-wise RNN (char-RNN)\n",
    "First let import some python module that we use though this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing training data\n",
    "We will train our char-RNN on Anna Karenina (anna.txt), we need to load our data into memory to get the character-sizes ($D$ and $C$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our input has 83 unique characters\n"
     ]
    }
   ],
   "source": [
    "text = None\n",
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# build set of all characters\n",
    "ch_set = set(text)\n",
    "ch_size = len(ch_set)\n",
    "print('our input has %d unique characters' % ch_size)\n",
    "\n",
    "# helper dictionary to map char -> index and inversely\n",
    "ch_to_int = {c:i for i,c in enumerate(ch_set)}\n",
    "int_to_ch = dict(enumerate(ch_set))\n",
    "\n",
    "# convert text to numerics input data\n",
    "datas = np.array([ch_to_int[c] for c in text], dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 18, 72, 61, 19, 52, 49, 60, 39, 44, 44, 44,  6, 72, 61, 61, 31,\n",
       "       60, 66, 72, 65, 50, 62, 50, 52, 58, 60, 72, 49, 52, 60, 72, 62, 62,\n",
       "       60, 72, 62, 50, 33, 52,  0, 60, 52, 16, 52, 49, 31, 60, 32, 81, 18,\n",
       "       72, 61, 61, 31, 60, 66, 72, 65, 50, 62, 31, 60, 50, 58, 60, 32, 81,\n",
       "       18, 72, 61, 61, 31, 60, 50, 81, 60, 50, 19, 58, 60, 40, 71, 81, 44,\n",
       "       71, 72, 31, 82, 44, 44, 55, 16, 52, 49, 31, 19, 18, 50, 81], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split up data into batches and into training & validation sets (using the split_frac argument). We are not making a test set here, instead we will test if our model can generate new text (that makes some sense).\n",
    "\n",
    "We use the 2D-matrix to store input-data and target-data where \n",
    "\n",
    "* each row is a long concatenated string from the character data\n",
    "* the number of rows is equal to the batch_size (number of sample in each batch)\n",
    "\n",
    "The idea is given **matrix_data**, to get a batch of samples of *seq_len*, we can have a sliding windows \n",
    "$$\\mathrm{matrix\\_data}[:, p:p+seq\\_len]$$\n",
    "which gives batch_size rows and each row is a sequence has seq_len characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 35725)\n",
      "(50, 3975)\n"
     ]
    }
   ],
   "source": [
    "def split_data(datas, batch_size, seq_len, split_frac = 0.9):\n",
    "    '''\n",
    "    Split datas into training and validation sets, inputs & targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ------------------------\n",
    "    datas : character input (in numerics format)\n",
    "    batch_size: size of each batch\n",
    "    seq_len: length of each sequence\n",
    "    split_frac: fraction of batches to keep in the training set\n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    '''\n",
    "    # get size of each batch & number of batches\n",
    "    slice_size = batch_size * seq_len\n",
    "    n_batches = len(datas) // slice_size\n",
    "    \n",
    "    # put data into x & y, note here y is x shifted to the right one\n",
    "    x = datas[:n_batches * slice_size]\n",
    "    y = datas[1: 1 + n_batches * slice_size]\n",
    "    \n",
    "    # split x & y to batch_size slices, then stack them into 2D matrix => each row is a sequence input\n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # split into training and validation sets\n",
    "    split_idx = int(n_batches * split_frac)\n",
    "    train_x, train_y = x[:, : split_idx * seq_len], y[:, : split_idx * seq_len]\n",
    "    val_x, val_y = x[:, split_idx * seq_len :], y[:, split_idx * seq_len :]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y\n",
    "    \n",
    "batch_size = 50\n",
    "seq_len = 25\n",
    "train_x, train_y, val_x, val_y = split_data(datas, batch_size, seq_len)\n",
    "\n",
    "print (train_x.shape)\n",
    "print (val_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "Let first implement the forward pass \n",
    "\n",
    "* denote $\\vec{x}=(x_1,\\ldots,x_n)$ the input-sequence\n",
    "* denote $\\vec{\\targets} = (\\targets_1,\\ldots,\\targets_n)$ the target-sequence\n",
    "* given $W_{xh}, W_{hh}, b_h, W_{hy}, b_y$,and initial hidden state $h_0$, \n",
    "\n",
    "we want to compute the scores output\n",
    "$$\n",
    "\\vec{y} = (y_1, \\ldots, y_n) \n",
    "$$\n",
    "and the Softmax loss\n",
    "$$\n",
    "L(\\vec{y},\\vec{\\targets}) = \\sum_{t} L_t(y_t, \\targets_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_forward_one_sample(x, targets, h_prev, params):\n",
    "    '''\n",
    "    Implements the forward pass for rnn\n",
    "    \n",
    "    Arguments\n",
    "    ------------------------------\n",
    "        x: input data\n",
    "        targets: input targets\n",
    "        h_prev: hidden state at t=0\n",
    "        params: a tuple of model parameters (Wxh, Whh, Why, bh, by)\n",
    "    Returns \n",
    "        loss: the softmax loss\n",
    "        cache: the cache to be used in backward pass\n",
    "    '''\n",
    "    seq_len = len(x)\n",
    "    Wxh, Whh, Why, bh, by = params\n",
    "    \n",
    "    # convert input to one-hot encoding\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    \n",
    "    # previous hidden state\n",
    "    hs[-1] = np.copy(h_prev)\n",
    "    \n",
    "    loss = 0.0\n",
    "    for t in range(seq_len):\n",
    "        # one-hot xs\n",
    "        xs[t] = np.zeros((ch_size, 1))\n",
    "        xs[t][x[t]] = 1.0\n",
    "        \n",
    "        # update hidden state\n",
    "        hs[t] = np.tanh(Wxh.dot(xs[t]) + Whh.dot(hs[t-1]) + bh)\n",
    "        \n",
    "        # update scores & probab\n",
    "        ys[t] = Why.dot(hs[t]) + by\n",
    "        tmp = np.exp(ys[t])\n",
    "        ps[t] = tmp/np.sum(tmp)\n",
    "        tmp = -np.log(ps[t][targets[t], 0])        \n",
    "        loss += tmp\n",
    "\n",
    "    cache = (xs, hs, ps, targets)\n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "h0 = np.zeros((hidden_size, 1))\n",
    "Wxh = np.random.randn(hidden_size, ch_size) * 1e-3\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 1e-3\n",
    "Why = np.random.randn(ch_size, hidden_size) * 1e-3\n",
    "bh = np.zeros((hidden_size,1))\n",
    "by = np.zeros((ch_size,1))\n",
    "params = (Wxh, Whh, Why, bh, by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.2564973924\n"
     ]
    }
   ],
   "source": [
    "test_len = 3\n",
    "loss, cache = rnn_forward_one_sample(train_x[0,0:test_len], train_y[0, 0: test_len], h0, params)\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation\n",
    "Note that the loss function is\n",
    "$$\n",
    "L = \\sum_t{L_t(y_t,\\targets_t)}\n",
    "$$\n",
    "So first we need to compute\n",
    "$$\n",
    "\\nabla_{y_t}L_t(y_t,\\targets_t)\n",
    "$$\n",
    "Recall from previous notebook on [Softmax loss](https://minh84.github.io/ml-examples/demos/learn_tf/02-linear-classifier-softmax/), we have\n",
    "$$\n",
    "\\nabla_{y_t}L_t(y_t,\\targets_t) = p_t - \\mathrm{eye}[\\targets_t]\n",
    "$$\n",
    "where $\\mathrm{eye}[\\targets_t]$ is $\\targets_t$-th row of the identity matrix (note here we use [np.eye](https://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html).\n",
    "\n",
    "Next backward-propagation from scores $y_t$ to hidden-state $h_t$, note that \n",
    "$$\n",
    "y_t = W_{hy} h_t + b_y\n",
    "$$\n",
    "which is a linear function, so we can derive the backward derivative on $h_t, W_{hy}, b_y$ as\n",
    "\\begin{align*}\n",
    "\\nabla_{h_t}L_t &= (W_{hy})^T \\times \\nabla_{y_t}L_t(y_t,\\targets_t)\\\\\n",
    "\\nabla_{W_{hy}}L_t &= \\nabla_{y_t}L_t(y_t,\\targets_t) \\times \\left(h_t\\right)^T\\\\\n",
    "\\nabla_{b}L_t &= \\nabla_{y_t}L_t(y_t,\\targets_t)\n",
    "\\end{align*}\n",
    "Note here since we share $W_{hy}, b_y$ accross $t$ the derivative on these term need to be added up.\n",
    "\n",
    "Now look at the hidden-state, since we have\n",
    "$$\n",
    "h_t = \\tanh(W_{xh} \\times x_t + W_{hh} \\times h_{t-1} + b_h)\n",
    "$$\n",
    "and $\\frac{\\partial \\tanh(x)}{\\partial x} = 1 - \\left(\\tanh(x)\\right)^2$\n",
    "\n",
    "Then given $\\nabla_{h_t}L_t$, we derive the following gradient\n",
    "\\begin{align*}\n",
    "\\nabla_{h_{t-1}}L_t &= (1.0 - h_t^2) \\odot \\left(W_{hh}\\right)^T\\times \\nabla_{h_t}L_t\\\\\n",
    "\\nabla_{W_{xh}}L_t &= (1.0 - h_t^2) \\odot \\nabla_{h_t}L_t \\times (x_t)^T\\\\\n",
    "\\nabla_{W_{hh}}L_t &= (1.0 - h_t^2) \\odot \\nabla_{h_t}L_t \\times (h_{t-1})^T\\\\\n",
    "\\nabla_{b_{h}}L_t &= (1.0 - h_t^2) \\odot \\nabla_{h_t}L_t\n",
    "\\end{align*}\n",
    "Note here we need actually compute\n",
    "$$\n",
    "\\nabla_{h_t} \\sum_{t}L_t\n",
    "$$\n",
    "And we notice that in forward pass $h_t$ only affect $L_k$ for $k\\geq t$, so at each time step $t$ we have two term\n",
    "$$\n",
    "\\nabla_{h_t} \\sum_{t}L_t = \\nabla_{h_t} L_t + \\nabla_{h_t} \\sum_{k>t}L_k\n",
    "$$\n",
    "And the second term has following recursive form\n",
    "$$\n",
    "\\nabla_{h_t} \\sum_{k>t}L_k = \\left\\{\n",
    "\\begin{array}{cl}\n",
    "0 & \\text{if } t=n\\\\\n",
    "(1.0 - h_{t+1}^2) \\odot \\left(W_{hh}\\right)^T\\times \\left(\\nabla_{h_{t+1}}L_{t+1} + \\nabla_{h_{t+1}} \\sum_{k>t+1}L_k\\right) & \\text{otherwise i.e } t < n\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "We will see in the code below\n",
    "\\begin{align*}\n",
    "\\nabla_{h_t} \\sum_{k>t}L_k &= \\textit{dhnext}\\\\\n",
    "\\nabla_{h_t} L_t &= \\textit{np.dot(Why.T, dy)}\\\\\n",
    "\\nabla_{h_t} L &= \\textit{np.dot(Why.T, dy) + dhnext}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_backward_one_sample(cache, params):\n",
    "    '''\n",
    "    Implement the backward propagation for rnn\n",
    "    \n",
    "    Arguments\n",
    "    ----------------------\n",
    "        cache = (xs, hs, ps, targets) \n",
    "        params = (Wxh, Whh, Why, bh, by)\n",
    "    \n",
    "    Return \n",
    "        dWxh, dWhh, dWhy, dbh, dby\n",
    "    '''\n",
    "    xs, hs, ps, targets = cache\n",
    "    seq_len = len(xs)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    Wxh, Whh, Why, bh, by = params\n",
    "    \n",
    "    dWxh = np.zeros_like(Wxh)\n",
    "    dWhh = np.zeros_like(Whh)\n",
    "    dWhy = np.zeros_like(Why)\n",
    "    dbh  = np.zeros_like(bh)\n",
    "    dby  = np.zeros_like(by)\n",
    "    dhs = []\n",
    "    for t in reversed(range(seq_len)):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backward to y see above formula\n",
    "        \n",
    "        # backward to W_hy & b_y and some accross t\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        \n",
    "        # backward to dy_t(L_t) -> dh_t (L_t) + the one from \\sum_{k>t}L_k\n",
    "        dh = np.dot(Why.T, dy) + dhnext\n",
    "        \n",
    "        dhs.append(dhnext)\n",
    "        \n",
    "        # backward tanh\n",
    "        dhraw = (1.0 - hs[t] * hs[t]) * dh\n",
    "        \n",
    "        # backward to bh, Wxh, Whh\n",
    "        dbh  += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        \n",
    "        # propagate dhnext for dh_{t-1}(\\sum_{k>=t}L_k)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    return dWxh, dWhh, dWhy, dbh, dby, np.array(dhs)\n",
    "\n",
    "def loss_rnn_one_sample(x, targets, h_prev, params):\n",
    "    '''\n",
    "    Implement one forward-pass compute the loss and one backward-pass to compute gradient\n",
    "    \n",
    "    Arguments\n",
    "    --------------------\n",
    "        x: input sequence\n",
    "        targets: target sequence\n",
    "        h_prev: previous hidden state\n",
    "        params: a tuple of (Wxh, Whh, Why, bh, by)\n",
    "    \n",
    "    Return\n",
    "        loss, dWxh, dWhh, dWhy, dbh, dby and hidden-state for next sequence\n",
    "    '''\n",
    "    seq_len = len(x)\n",
    "    loss, cache = rnn_forward_one_sample(x, targets, h_prev, params)\n",
    "    dWxh, dWhh, dWhy, dbh, dby, dhs = rnn_backward_one_sample(cache, params)\n",
    "    hs = cache[1]\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[seq_len - 1], dhs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical testing\n",
    "It's always go idea to test our implementation via numerical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we reduce number of hidden_size so that we can test it quicker\n",
    "hidden_size = 10\n",
    "test_len = 3\n",
    "\n",
    "h0 = np.zeros((hidden_size, 1))\n",
    "Wxh = np.random.randn(hidden_size, ch_size) \n",
    "Whh = np.random.randn(hidden_size, hidden_size)\n",
    "Why = np.random.randn(ch_size, hidden_size)\n",
    "bh = np.zeros((hidden_size,1))\n",
    "by = np.zeros((ch_size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:   31.98509\n",
      "Rel error dWxh: 1.19144e-06\n",
      "Rel error dWhh: 5.05316e-08\n",
      "Rel error dWhy: 3.42830e-05\n",
      "Rel error dbh:  3.11615e-10\n",
      "Rel error dby:  5.52779e-06\n"
     ]
    }
   ],
   "source": [
    "from gradient_check import grad_check_sparse, eval_numerical_gradient, rel_error\n",
    "\n",
    "params = (Wxh, Whh, Why, bh, by)\n",
    "\n",
    "x, targets = train_x[0,0:test_len], train_y[0, 0: test_len]\n",
    "loss, dWxh, dWhh, dWhy, dbh, dby, hnext,_ = loss_rnn_one_sample(x, targets, h0, params)\n",
    "\n",
    "f = lambda wxh : loss_rnn_one_sample(x, targets, h0, (wxh, Whh, Why, bh, by))[0]\n",
    "grad_Wxh = eval_numerical_gradient(f, Wxh, verbose = False)\n",
    "\n",
    "f = lambda whh : loss_rnn_one_sample(x, targets, h0, (Wxh, whh, Why, bh, by))[0]\n",
    "grad_Whh = eval_numerical_gradient(f, Whh, verbose = False)\n",
    "\n",
    "f = lambda why : loss_rnn_one_sample(x, targets, h0, (Wxh, Whh, why, bh, by))[0]\n",
    "grad_Why = eval_numerical_gradient(f, Why, verbose = False)\n",
    "\n",
    "f = lambda b_h : loss_rnn_one_sample(x, targets, h0, (Wxh, Whh, Why, b_h, by))[0]\n",
    "grad_bh = eval_numerical_gradient(f, bh, verbose = False)\n",
    "\n",
    "f = lambda b_y : loss_rnn_one_sample(x, targets, h0, (Wxh, Whh, Why, bh, b_y))[0]\n",
    "grad_by = eval_numerical_gradient(f, by, verbose = False)\n",
    "\n",
    "print ('Loss: {:10.5f}'.format(loss))\n",
    "print('Rel error dWxh: {:10.5e}'.format(rel_error(dWxh, grad_Wxh)))\n",
    "print('Rel error dWhh: {:10.5e}'.format(rel_error(dWhh, grad_Whh)))\n",
    "print('Rel error dWhy: {:10.5e}'.format(rel_error(dWhy, grad_Why)))\n",
    "print('Rel error dbh:  {:10.5e}'.format(rel_error(dbh, grad_bh)))\n",
    "print('Rel error dby:  {:10.5e}'.format(rel_error(dby, grad_by)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look correct, to double-check it, now let's try to implement char-RNN in TensorFlows. Also TensorFlows allows us to extend RNN very well.\n",
    "\n",
    "## Vanilla char-RNN with TensorFlow\n",
    "In this part we will learn how to\n",
    "\n",
    "* Use one-hot encoding in TensorFlow with [tf.one_hot](https://www.tensorflow.org/api_docs/python/tf/one_hot)\n",
    "* Split/concat array data with [tf.split](https://www.tensorflow.org/api_docs/python/tf/split)/[tf.concat](https://www.tensorflow.org/api_docs/python/tf/concat)\n",
    "* Implement rnn forward/backward for one sample in TensorFlows, then look at two issue with vanilla-RNN\n",
    "    * Gradient exploding\n",
    "    * Gradient vanishing\n",
    "* Implement rnn with mini-batches using [tf.contrib.rnn.BasicRNNCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicRNNCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  31.9850876069\n",
      "Rel error loss: 0.00000e+00\n",
      "Rel error dWxh: 1.05926e-12\n",
      "Rel error dWxh: 3.94742e-14\n",
      "Rel error dWxh: 9.02872e-14\n",
      "Rel error dWxh: 4.93427e-16\n",
      "Rel error dWxh: 8.92087e-16\n"
     ]
    }
   ],
   "source": [
    "# reset default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seq_len = test_len\n",
    "\n",
    "# define input placeholder and tensor h0\n",
    "vx = tf.placeholder(tf.int32, [seq_len], name = 'input')\n",
    "vy = tf.placeholder(tf.int32, [seq_len], name = 'target')\n",
    "vh0 = tf.zeros([hidden_size, 1], dtype=tf.float64)\n",
    "\n",
    "# one hot encoding\n",
    "vx_one_hot = tf.one_hot(vx, ch_size, dtype=tf.float64)\n",
    "\n",
    "# define basic rnn\n",
    "cell = tf.contrib.rnn.BasicRNNCell(hidden_size)\n",
    "\n",
    "### Run the data through the RNN layers\n",
    "# This makes a list where each element is on step in the sequence\n",
    "rnn_inputs = tf.split(vx_one_hot, seq_len, axis=0)\n",
    "\n",
    "with tf.variable_scope('hidden'):\n",
    "    vWxh = tf.Variable(Wxh, name = 'Wxh', dtype=tf.float64)\n",
    "    vWhh = tf.Variable(Whh, name = 'Whh', dtype=tf.float64)\n",
    "    vbh = tf.Variable(bh, name = 'bh', dtype=tf.float64)    \n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    vWhy = tf.Variable(Why, name = 'Why', dtype=tf.float64)\n",
    "    vby = tf.Variable(by, name = 'by', dtype=tf.float64)\n",
    "\n",
    "vhs_t = vh0    \n",
    "ys = []\n",
    "for t, xs_t in enumerate(rnn_inputs):\n",
    "    # we need transpose xs_t since TensorFlow takes vector-input by row, here we want column with shape (ch_size, 1)\n",
    "    vhs_t = tf.tanh(tf.matmul(vWxh, tf.transpose(xs_t)) + tf.matmul(vWhh, vhs_t) + vbh)\n",
    "    \n",
    "    # the form is exactly as in numpy's implementation\n",
    "    ys_t = tf.matmul(vWhy, vhs_t) + vby    \n",
    "    ys.append(ys_t)\n",
    "\n",
    "# we need to stack (ys_t)^T in each row so that we can use tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "output = tf.transpose(tf.concat(ys,axis=1))\n",
    "\n",
    "# get the Softmax and gradient\n",
    "cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = output, labels=vy))\n",
    "grad = tf.gradients(cost, [vWxh, vWhh, vWhy, vbh, vby])\n",
    "\n",
    "# run test\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    loss_tf, grad_tf = sess.run([cost, grad], feed_dict = {vx : x, vy : targets})   \n",
    "    print ('loss: ', loss_tf)\n",
    "    print('Rel error loss: {:10.5e}'.format(rel_error(loss, loss_tf)))\n",
    "    print('Rel error dWxh: {:10.5e}'.format(rel_error(dWxh, grad_tf[0])))\n",
    "    print('Rel error dWxh: {:10.5e}'.format(rel_error(dWhh, grad_tf[1])))\n",
    "    print('Rel error dWxh: {:10.5e}'.format(rel_error(dWhy, grad_tf[2])))\n",
    "    print('Rel error dWxh: {:10.5e}'.format(rel_error(dbh, grad_tf[3])))\n",
    "    print('Rel error dWxh: {:10.5e}'.format(rel_error(dby, grad_tf[4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For single sample, we can see that the results obtained by TensorFlow are very close to result obtained by numpy as expected.\n",
    "\n",
    "Now let's look at two issues with Vanilla-RNN\n",
    "\n",
    "* Gradient exploding\n",
    "* Gradient vanishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_hidden_size = 3\n",
    "\n",
    "h0 = np.zeros((test_hidden_size, 1))\n",
    "test_Wxh = np.zeros((test_hidden_size, ch_size))\n",
    "test_Wxh[range(test_hidden_size), range(test_hidden_size)] = 1.0\n",
    "test_Why = np.zeros((ch_size, test_hidden_size))\n",
    "test_Why[range(test_hidden_size), range(test_hidden_size)] = 1.0\n",
    "\n",
    "test_bh = np.zeros((test_hidden_size,1))\n",
    "test_by = np.zeros((ch_size,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHdCAYAAAAAdE2GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMACFE0AAAv4/7E+lPj0V1LNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5154710710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAHVCAYAAABBptdCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f51541dcb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f51540ef668>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAHVCAYAAAAzabX0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMBNCCsAAAA3+X8KlPrd2XsqWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f518ff645c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_Whh = np.diag([2., 0.5, 1.0])\n",
    "test_params = (test_Wxh, test_Whh, test_Why, test_bh,test_by)\n",
    "test_lens = []\n",
    "dhs0 = []\n",
    "dhs1 = []\n",
    "dhs2 = []\n",
    "for sl in range(5,50):\n",
    "    test_lens.append(sl)\n",
    "    test_x, test_targets = train_x[0,0:sl], train_y[0, 0: sl]\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hnext, dhs = loss_rnn_one_sample(test_x, test_targets, h0, test_params)\n",
    "    dhs0.append(np.abs(dhs[0]))\n",
    "    dhs1.append(np.abs(dhs[1]))\n",
    "    dhs2.append(np.abs(dhs[2]))\n",
    "\n",
    "plt.plot(test_lens, dhs0)\n",
    "plt.show()\n",
    "plt.plot(test_lens, dhs1)\n",
    "plt.show()\n",
    "plt.plot(test_lens, dhs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we vary the length of input sequence and draw the gradient with respect to $h_0$. We set $W_hh$ is a diagonal matrix with first eigenvalue > 1.0, the second < 1.0 and the third = 1.0\n",
    "\n",
    "From the plot, we see \n",
    "* gradient for $h_0[0]$ explode with respect to sequence-length => exploding Gradient\n",
    "* gradient for $h_0[1]$ become constant with respect to sequence-length => vanishing Gradient\n",
    "* gradient for $h_0[2]$ is normal since we set the third eigen-value = 1.0\n",
    "\n",
    "One can use gradient clipping to control exploding and LSTM to cure vanishing Gradient (in this note we only discuss RNN). \n",
    "\n",
    "## Mini-batches for Vanilla char-RNN with TensorFlow\n",
    "Let's implement gradient-clipping with TensorFlow and we will also implement mini-batches version instead of single input. Before doing the mini-batches version we need re-define the shape of our data\n",
    "\n",
    "* input_data: has shape (batch_size, seq_len) with each point is an int32\n",
    "* target_data: has same format as input_data\n",
    "\n",
    "At each time step $t$, instead of computing single hidden-state $h_t$ & single score $y_t$, we will update a vector of batch_size $h_t$ and $y_t$. We will learn how to\n",
    "\n",
    "* use [tf.squeeze](https://www.tensorflow.org/api_docs/python/tf/squeeze) to remove dimension of size 1, so that we can use matrix multiplication\n",
    "* use [tf.contrib.rnn.BasicRNNCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicRNNCell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mini_batch_rnn(ch_size, batch_size = 50, seq_len = 25, hidden_size = 128, \n",
    "                   use_lstm = False, use_dropout = False, num_layers = 1,\n",
    "                   lr = 1e-3, grad_clip = 5.0, dtype = tf.float32):\n",
    "    '''\n",
    "    This implement mini-batch version for rnn\n",
    "    \n",
    "    Arguments\n",
    "    ---------------------\n",
    "        ch_size: size of all character in our training set\n",
    "        batch_size: number of input sequence per batch\n",
    "        seq_len: len of input sequence\n",
    "        hidden_size: size of hidden-state\n",
    "        lr: learning rate for optimizer\n",
    "        grad_clip: to clip the gradient to control exploding gradient\n",
    "    '''\n",
    "    # reset the graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # create placeholders\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, seq_len], name = 'inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, seq_len], name = 'targets')    \n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(dtype=dtype, name='keep_prob')\n",
    "    \n",
    "    # one-hot encoding\n",
    "    inputs_one_hot = tf.one_hot(inputs, ch_size, dtype = dtype) # this has shape (batch_size, seq_len, ch_size)\n",
    "    \n",
    "    # create rnn_inputs\n",
    "    # rnn_inputs[t] is inputs[:, t] in one-hot encoding\n",
    "    rnn_inputs = [tf.squeeze(i, axis=[1]) for i in tf.split(inputs_one_hot, seq_len, axis=1)]\n",
    "    \n",
    "    # create RNN layer + dropout\n",
    "    if use_lstm:\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n",
    "    else:\n",
    "        cell = tf.contrib.rnn.BasicRNNCell(hidden_size)\n",
    "    if use_dropout:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "        \n",
    "    # stack up multiple RNN layers + get initial_state\n",
    "    if num_layers > 1:\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, dtype)\n",
    "    \n",
    "    # compute h_t update via static_rnn: outputs is [h_t, t=1,...,n], final_state = h_n\n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "        \n",
    "    # each outputs is h_t of size (batch_size, hidden_size) and len(outputs) = seq_len\n",
    "    # we concatenate by axis=1 so row i-th is stack h_1[i],...,h_n[i]\n",
    "    seq_output = tf.concat(outputs, axis=1)            # this has shape (batch_size, hidden_size * seq_len)\n",
    "    \n",
    "    # reshape so we can use matmul to produce y_t\n",
    "    output = tf.reshape(seq_output, [-1, hidden_size]) \n",
    "    \n",
    "    # create Why & by\n",
    "    with tf.variable_scope('softmax'):\n",
    "        vWhy = tf.Variable(tf.truncated_normal((hidden_size, ch_size), stddev=0.1, dtype = dtype), name = 'Why')\n",
    "        vby = tf.Variable(tf.zeros(ch_size, dtype = dtype), name = 'by', dtype = dtype)\n",
    "    \n",
    "    # compute y_t\n",
    "    logits = tf.matmul(output, vWhy) + vby\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # reshape target so we can use Softmax loss\n",
    "    targets_reshaped = tf.reshape(targets, [-1])\n",
    "    cost = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, \n",
    "                                                                        labels = targets_reshaped))\n",
    "    \n",
    "    # we get trainable variables and clipped-gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads_raw = tf.gradients(cost, tvars)\n",
    "    grads, _ = tf.clip_by_global_norm(grads_raw, grad_clip)\n",
    "    \n",
    "    # we use AdamOptimizer so that learning_rate can be auto-ajust\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    \n",
    "    # we apply the gradient onto trainable variables note that we have already clipped gradients\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # export the nodes\n",
    "    export_nodes = ['inputs', 'targets', 'keep_prob', 'initial_state', 'grads_raw', 'final_state', 'cost', 'preds', 'optimizer']\n",
    "    local_dict = locals()\n",
    "    \n",
    "    # use namedtuple to export nodes\n",
    "    VanillaRRN = namedtuple('VanillaRNN', export_nodes)\n",
    "    model = VanillaRRN(*[local_dict[node] for node in export_nodes])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model\n",
    "Since we have well tested base-model, it's always good idea to check our implementation v.s base-one. Note that in \n",
    "*BasicRNNCell*, the weights $W_{xh}$ and $W_{hh}$ are stacked together. The code below is a bit ugly since we have to hack the the initilization steps, but it works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.9850876069\n",
      "Rel error loss: 0.00000e+00\n",
      "Rel error dWxh: 1.05920e-12\n",
      "Rel error dWxh: 1.48028e-15\n",
      "Rel error dWxh: 3.55016e-14\n",
      "Rel error dby:  7.68366e-16\n"
     ]
    }
   ],
   "source": [
    "model = mini_batch_rnn(ch_size, batch_size=1, seq_len = 3, hidden_size=10, dtype = tf.float64)\n",
    "\n",
    "wxh_test = np.hstack([Wxh, Whh]).transpose()\n",
    "btest = bh.reshape(hidden_size,)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for v in tf.trainable_variables():        \n",
    "        if (v.name == 'rnn/basic_rnn_cell/weights:0'):\n",
    "            _ = sess.run(tf.assign(v, wxh_test))\n",
    "        if (v.name == 'rnn/basic_rnn_cell/biases:0'):\n",
    "            _ = sess.run(tf.assign(v, btest))\n",
    "        \n",
    "        if (v.name == 'softmax/Why:0'):\n",
    "            _ = sess.run(tf.assign(v, Why.transpose()))\n",
    "            \n",
    "        if (v.name == 'softmax/by:0'):\n",
    "            _ = sess.run(tf.assign(v, by.reshape(ch_size,)))\n",
    "    \n",
    "    loss_rnn, grads_raw = sess.run([model.cost, model.grads_raw], \n",
    "                                   feed_dict = {model.inputs: x.reshape(1,3),\n",
    "                                                model.targets: targets.reshape(1,3),\n",
    "                                                model.keep_prob : 1.0})\n",
    "    print (loss_rnn)\n",
    "    \n",
    "    print('Rel error loss: {:10.5e}'.format(rel_error(loss, loss_rnn)))\n",
    "    print('Rel error dWxh: {:10.5e}'.format(rel_error(np.hstack([dWxh, dWhh]).transpose(), grads_raw[0])))    \n",
    "    print('Rel error dWxh: {:10.5e}'.format(rel_error(dbh.reshape(hidden_size,), grads_raw[1])))\n",
    "    print('Rel error dWxh: {:10.5e}'.format(rel_error(dWhy.transpose(), grads_raw[2])))\n",
    "    \n",
    "    print('Rel error dby:  {:10.5e}'.format(rel_error(dby.reshape(ch_size,), grads_raw[3])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative errors are quite small, we are confident that it works as expected. Let's train our model\n",
    "\n",
    "## Training time\n",
    "Each time-step, we feed our model as mini-batch of data. It will update training variables and return a state back (*final_state*) in to the network so the next batch can continue the state from the previous batch. Note that, we have to\n",
    "\n",
    "* At each epoch, we have to reset initial_state back to 0.0\n",
    "* At each time we valuate on validation set, we have to reset initial_state back to 0.0\n",
    "\n",
    "We also save down a checkpoint so that we can re-use later, we use the following naming convention (taken from [Udacity](https://www.udacity.com/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "seq_len = 25\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "keep_prob = 0.5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def get_batch(arrs, seq_len):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/seq_len)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*seq_len: (b+1)*seq_len] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 100/28580 Training loss: 4136.5017 0.0126 sec/batch\n",
      "Epoch 1/20  Iteration 200/28580 Training loss: 3910.3344 0.0118 sec/batch\n",
      "Epoch 1/20  Iteration 300/28580 Training loss: 3731.1911 0.0140 sec/batch\n",
      "Epoch 1/20  Iteration 400/28580 Training loss: 3598.1606 0.0127 sec/batch\n",
      "Epoch 1/20  Iteration 500/28580 Training loss: 3496.4278 0.0185 sec/batch\n",
      "Validation loss: 2980.15 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 600/28580 Training loss: 3417.7046 0.0151 sec/batch\n",
      "Epoch 1/20  Iteration 700/28580 Training loss: 3354.5244 0.0120 sec/batch\n",
      "Epoch 1/20  Iteration 800/28580 Training loss: 3302.7196 0.0120 sec/batch\n",
      "Epoch 1/20  Iteration 900/28580 Training loss: 3255.9259 0.0147 sec/batch\n",
      "Epoch 1/20  Iteration 1000/28580 Training loss: 3215.8239 0.0118 sec/batch\n",
      "Validation loss: 2746.1 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 1100/28580 Training loss: 3178.6686 0.0128 sec/batch\n",
      "Epoch 1/20  Iteration 1200/28580 Training loss: 3148.1123 0.0120 sec/batch\n",
      "Epoch 1/20  Iteration 1300/28580 Training loss: 3120.4339 0.0121 sec/batch\n",
      "Epoch 1/20  Iteration 1400/28580 Training loss: 3094.3668 0.0189 sec/batch\n",
      "Epoch 2/20  Iteration 1500/28580 Training loss: 2724.6090 0.0119 sec/batch\n",
      "Validation loss: 2590.49 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 1600/28580 Training loss: 2711.4251 0.0170 sec/batch\n",
      "Epoch 2/20  Iteration 1700/28580 Training loss: 2691.6595 0.0222 sec/batch\n",
      "Epoch 2/20  Iteration 1800/28580 Training loss: 2676.3410 0.0168 sec/batch\n",
      "Epoch 2/20  Iteration 1900/28580 Training loss: 2660.6824 0.0115 sec/batch\n",
      "Epoch 2/20  Iteration 2000/28580 Training loss: 2651.0783 0.0118 sec/batch\n",
      "Validation loss: 2470.0 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 2100/28580 Training loss: 2641.3160 0.0181 sec/batch\n",
      "Epoch 2/20  Iteration 2200/28580 Training loss: 2632.9087 0.0120 sec/batch\n",
      "Epoch 2/20  Iteration 2300/28580 Training loss: 2623.2458 0.0124 sec/batch\n",
      "Epoch 2/20  Iteration 2400/28580 Training loss: 2614.0786 0.0159 sec/batch\n",
      "Epoch 2/20  Iteration 2500/28580 Training loss: 2606.1087 0.0118 sec/batch\n",
      "Validation loss: 2393.02 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 2600/28580 Training loss: 2597.0415 0.0118 sec/batch\n",
      "Epoch 2/20  Iteration 2700/28580 Training loss: 2590.5758 0.0162 sec/batch\n",
      "Epoch 2/20  Iteration 2800/28580 Training loss: 2584.2045 0.0136 sec/batch\n",
      "Epoch 3/20  Iteration 2900/28580 Training loss: 2465.2297 0.0138 sec/batch\n",
      "Epoch 3/20  Iteration 3000/28580 Training loss: 2459.2498 0.0195 sec/batch\n",
      "Validation loss: 2311.77 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 3100/28580 Training loss: 2452.3976 0.0118 sec/batch\n",
      "Epoch 3/20  Iteration 3200/28580 Training loss: 2443.2038 0.0120 sec/batch\n",
      "Epoch 3/20  Iteration 3300/28580 Training loss: 2431.3037 0.0120 sec/batch\n",
      "Epoch 3/20  Iteration 3400/28580 Training loss: 2426.5805 0.0123 sec/batch\n",
      "Epoch 3/20  Iteration 3500/28580 Training loss: 2419.8920 0.0179 sec/batch\n",
      "Validation loss: 2255.9 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 3600/28580 Training loss: 2415.8266 0.0118 sec/batch\n",
      "Epoch 3/20  Iteration 3700/28580 Training loss: 2409.4420 0.0179 sec/batch\n",
      "Epoch 3/20  Iteration 3800/28580 Training loss: 2404.9310 0.0117 sec/batch\n",
      "Epoch 3/20  Iteration 3900/28580 Training loss: 2402.0643 0.0117 sec/batch\n",
      "Epoch 3/20  Iteration 4000/28580 Training loss: 2396.9839 0.0176 sec/batch\n",
      "Validation loss: 2213.69 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 4100/28580 Training loss: 2394.0830 0.0124 sec/batch\n",
      "Epoch 3/20  Iteration 4200/28580 Training loss: 2391.5387 0.0121 sec/batch\n",
      "Epoch 4/20  Iteration 4300/28580 Training loss: 2334.2537 0.0165 sec/batch\n",
      "Epoch 4/20  Iteration 4400/28580 Training loss: 2322.9250 0.0116 sec/batch\n",
      "Epoch 4/20  Iteration 4500/28580 Training loss: 2320.5922 0.0120 sec/batch\n",
      "Validation loss: 2161.09 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 4600/28580 Training loss: 2310.4793 0.0131 sec/batch\n",
      "Epoch 4/20  Iteration 4700/28580 Training loss: 2301.2519 0.0139 sec/batch\n",
      "Epoch 4/20  Iteration 4800/28580 Training loss: 2296.2580 0.0177 sec/batch\n",
      "Epoch 4/20  Iteration 4900/28580 Training loss: 2291.6483 0.0120 sec/batch\n",
      "Epoch 4/20  Iteration 5000/28580 Training loss: 2290.0260 0.0127 sec/batch\n",
      "Validation loss: 2127.39 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 5100/28580 Training loss: 2286.6523 0.0174 sec/batch\n",
      "Epoch 4/20  Iteration 5200/28580 Training loss: 2283.3672 0.0185 sec/batch\n",
      "Epoch 4/20  Iteration 5300/28580 Training loss: 2281.9765 0.0128 sec/batch\n",
      "Epoch 4/20  Iteration 5400/28580 Training loss: 2279.6089 0.0161 sec/batch\n",
      "Epoch 4/20  Iteration 5500/28580 Training loss: 2277.0570 0.0145 sec/batch\n",
      "Validation loss: 2101.16 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 5600/28580 Training loss: 2276.6697 0.0120 sec/batch\n",
      "Epoch 4/20  Iteration 5700/28580 Training loss: 2274.8026 0.0118 sec/batch\n",
      "Epoch 5/20  Iteration 5800/28580 Training loss: 2231.7913 0.0128 sec/batch\n",
      "Epoch 5/20  Iteration 5900/28580 Training loss: 2229.2160 0.0113 sec/batch\n",
      "Epoch 5/20  Iteration 6000/28580 Training loss: 2220.6815 0.0122 sec/batch\n",
      "Validation loss: 2068.08 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 6100/28580 Training loss: 2212.7822 0.0159 sec/batch\n",
      "Epoch 5/20  Iteration 6200/28580 Training loss: 2207.3413 0.0116 sec/batch\n",
      "Epoch 5/20  Iteration 6300/28580 Training loss: 2204.9479 0.0160 sec/batch\n",
      "Epoch 5/20  Iteration 6400/28580 Training loss: 2202.4336 0.0146 sec/batch\n",
      "Epoch 5/20  Iteration 6500/28580 Training loss: 2202.0247 0.0128 sec/batch\n",
      "Validation loss: 2056.77 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 6600/28580 Training loss: 2199.5982 0.0122 sec/batch\n",
      "Epoch 5/20  Iteration 6700/28580 Training loss: 2199.1817 0.0125 sec/batch\n",
      "Epoch 5/20  Iteration 6800/28580 Training loss: 2198.9645 0.0116 sec/batch\n",
      "Epoch 5/20  Iteration 6900/28580 Training loss: 2196.2707 0.0115 sec/batch\n",
      "Epoch 5/20  Iteration 7000/28580 Training loss: 2195.9998 0.0141 sec/batch\n",
      "Validation loss: 2026.71 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 7100/28580 Training loss: 2195.7634 0.0119 sec/batch\n",
      "Epoch 6/20  Iteration 7200/28580 Training loss: 2166.7306 0.0125 sec/batch\n",
      "Epoch 6/20  Iteration 7300/28580 Training loss: 2171.3913 0.0119 sec/batch\n",
      "Epoch 6/20  Iteration 7400/28580 Training loss: 2162.1012 0.0117 sec/batch\n",
      "Epoch 6/20  Iteration 7500/28580 Training loss: 2154.3448 0.0167 sec/batch\n",
      "Validation loss: 2006.71 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 7600/28580 Training loss: 2147.2826 0.0158 sec/batch\n",
      "Epoch 6/20  Iteration 7700/28580 Training loss: 2146.2887 0.0150 sec/batch\n",
      "Epoch 6/20  Iteration 7800/28580 Training loss: 2142.6955 0.0118 sec/batch\n",
      "Epoch 6/20  Iteration 7900/28580 Training loss: 2142.1725 0.0117 sec/batch\n",
      "Epoch 6/20  Iteration 8000/28580 Training loss: 2140.1554 0.0120 sec/batch\n",
      "Validation loss: 1992.19 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 8100/28580 Training loss: 2139.4993 0.0132 sec/batch\n",
      "Epoch 6/20  Iteration 8200/28580 Training loss: 2140.3230 0.0125 sec/batch\n",
      "Epoch 6/20  Iteration 8300/28580 Training loss: 2138.1994 0.0178 sec/batch\n",
      "Epoch 6/20  Iteration 8400/28580 Training loss: 2138.1179 0.0166 sec/batch\n",
      "Epoch 6/20  Iteration 8500/28580 Training loss: 2138.9479 0.0119 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f903cafab2e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m                             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                             model.initial_state: new_state}\n\u001b[0;32m---> 49\u001b[0;31m                     \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                     \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/build_tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/build_tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/build_tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/build_tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minhvu/workplaces/infra/anaconda3/envs/build_tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = mini_batch_rnn(ch_size, batch_size=batch_size, \n",
    "                       seq_len = seq_len, hidden_size=hidden_size, \n",
    "                       use_lstm = True, use_dropout=True, num_layers = 1, \n",
    "                       lr = learning_rate)\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "epochs = 20\n",
    "save_every_n = 500\n",
    "print_every = 100\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/seq_len)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], seq_len), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            if (iteration % print_every == 0):\n",
    "                print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                      'Iteration {}/{}'.format(iteration, iterations),\n",
    "                      'Training loss: {:.4f}'.format(loss/b),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], seq_len):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"./checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, hidden_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c\n",
    "\n",
    "def sample(checkpoint, n_samples, hidden_size, ch_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = mini_batch_rnn(ch_size, use_lstm=True, use_dropout=True, num_layers=1, batch_size = 1, seq_len = 1, hidden_size=hidden_size)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = ch_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, ch_size)\n",
    "        samples.append(int_to_ch[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, ch_size)\n",
    "            samples.append(int_to_ch[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The askander one to have as he said the missed her a cannabing, standed\n",
      "ofter that he was seet her at the storse, though she heard talking his troat, she had been to thim to say, which all had the some st\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i28500_l128_v1753.467.ckpt'\n",
    "samp = sample(checkpoint, 200, hidden_size, ch_size, prime=\"The\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
