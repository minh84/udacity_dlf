{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network in TensorFlow\n",
    "\n",
    "In the following, we'll use TensorFlow to classify the letters in the MNIST database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting .\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting .\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting .\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting .\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('.', one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST database\n",
    "Let's inspect the data a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training point: 55000\n",
      "shape of training data:   (55000, 28, 28, 1)\n",
      "labels histogram\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Container object of 10 artists>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIEkdMwQkqWOGgCR17H8AQLqdrlL5MOkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1450e95dcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('number of training point:', mnist.train.num_examples)\n",
    "print('shape of training data:  ', mnist.train.images.shape)\n",
    "labels = np.argwhere(mnist.train.labels==1)[:,1]\n",
    "hist, bins = np.histogram(labels, bins=10)\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "print('labels histogram')\n",
    "plt.bar(center, hist, align='center', width=width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters\n",
    "We define some global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# alway reset everything\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# optimize parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784     # 28 x 28 (img shape: 28 * 28 * 1)\n",
    "n_classes = 10    # mnist it to classify 0->9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network parameters\n",
    "We define some parameter/variables for our neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_hidden_layer = 256 # number of neuron in hidden layer\n",
    "\n",
    "weights = {'hidden_layer' : tf.Variable(tf.random_normal([n_input, n_hidden_layer], name = 'W_0')),\n",
    "           'out' : tf.Variable(tf.random_normal([n_hidden_layer, n_classes]), name = 'W_1')}\n",
    "\n",
    "biases = {'hidden_layer' : tf.Variable(tf.random_normal([n_hidden_layer]), name = 'b_0'),\n",
    "          'out' : tf.Variable(tf.random_normal([n_classes]), name = 'b_1')}\n",
    "\n",
    "# we define input and flatten it\n",
    "x = tf.placeholder('float', [None, 28,28,1], name = 'input_x')\n",
    "y = tf.placeholder('float', [None, n_classes], name = 'target_y')\n",
    "x_flat = tf.reshape(x,[-1, n_input], name = 'input_x_flat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron with ReLU\n",
    "We define our layer with ReLU as following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# affine layer\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
    "\n",
    "# relu layer\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "# output layer\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loss function and Optimizer\n",
    "The logists with be size of N x n_classes where N = number of samples and n_classes = 10. We need to define a loss function and choose an optimizer. We use the following loss $ L = \\frac{1}{N}\\sum L_i$ where $L_i$ is given by\n",
    "$$L_i = \\sum_{j=0}^{9}  -\\log(\\mathrm{logits}[i,j])\\times y[i,j]$$\n",
    "\n",
    "Regarding the optimizer, we use the simple Gradient Descent to minimize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# we define accuracy to measure the performance\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(logits,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out\n",
    "We are ready to run it and measure the performance inside each loop. After the model is trained, we save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 train-acc 0.378 vs val-acc 0.383\n",
      "Epoch   1 train-acc 0.532 vs val-acc 0.538\n",
      "Epoch   2 train-acc 0.611 vs val-acc 0.626\n",
      "Epoch   3 train-acc 0.661 vs val-acc 0.677\n",
      "Epoch   4 train-acc 0.692 vs val-acc 0.709\n",
      "Epoch   5 train-acc 0.716 vs val-acc 0.729\n",
      "Epoch   6 train-acc 0.735 vs val-acc 0.747\n",
      "Epoch   7 train-acc 0.750 vs val-acc 0.760\n",
      "Epoch   8 train-acc 0.762 vs val-acc 0.774\n",
      "Epoch   9 train-acc 0.772 vs val-acc 0.782\n",
      "saved x:      input_x:0\n",
      "saved y:      target_y:0\n",
      "saved x_flat: input_x_flat:0\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)  # we need to run this to initialize all variables\n",
    "\n",
    "# training loop\n",
    "num_batch_per_epoch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    for i in range(num_batch_per_epoch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)   # get batch sample\n",
    "        sess.run(optimizer, feed_dict = {x : batch_x, y : batch_y})\n",
    "    \n",
    "    train_acc = sess.run(accuracy, feed_dict={x: mnist.train.images, y: mnist.train.labels})\n",
    "    val_acc = sess.run(accuracy, feed_dict={x: mnist.validation.images, y: mnist.validation.labels})\n",
    "    \n",
    "    print('Epoch %3d train-acc %5.3f vs val-acc %5.3f' % (epoch, train_acc, val_acc))\n",
    "\n",
    "save_file = './tf_mnist_trained/model.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, save_file)\n",
    "\n",
    "print('saved x:      {}'.format(x.name))\n",
    "print('saved y:      {}'.format(y.name))\n",
    "print('saved x_flat: {}'.format(x_flat.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure the accuracy\n",
    "We need to measure the accuracy in the separated test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 0.783\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on test-set: %5.3f' % sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model\n",
    "We can reload the model from file. Note that, we need to ensure the name is matched and we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name x:      input_x:0\n",
      "name y:      target_y:0\n",
      "name x_flat: input_x_flat:0\n",
      "Test Accuracy: 0.7825999855995178\n"
     ]
    }
   ],
   "source": [
    "# reset everything\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_hidden_layer = 256 # number of neuron in hidden layer\n",
    "\n",
    "weights = {'hidden_layer' : tf.Variable(tf.random_normal([n_input, n_hidden_layer], name = 'W_0')),\n",
    "           'out' : tf.Variable(tf.random_normal([n_hidden_layer, n_classes]), name = 'W_1')}\n",
    "\n",
    "biases = {'hidden_layer' : tf.Variable(tf.random_normal([n_hidden_layer]), name = 'b_0'),\n",
    "          'out' : tf.Variable(tf.random_normal([n_classes]), name = 'b_1')}\n",
    "\n",
    "# we define input and flatten it\n",
    "x = tf.placeholder('float', [None, 28,28,1], name = 'input_x')\n",
    "y = tf.placeholder('float', [None, n_classes], name = 'target_y')\n",
    "x_flat = tf.reshape(x,[-1, n_input], name = 'input_x_flat')\n",
    "\n",
    "print('name x:      {}'.format(x.name))\n",
    "print('name y:      {}'.format(y.name))\n",
    "print('name x_flat: {}'.format(x_flat.name))\n",
    "\n",
    "# affine layer\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
    "\n",
    "# relu layer\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "# output layer\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n",
    "\n",
    "# we define accuracy to measure the performance\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(logits,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}