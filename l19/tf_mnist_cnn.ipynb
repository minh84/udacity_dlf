{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network in TensorFlow\n",
    "In this notebook, we look at an example of CNN in TensorFlow. We will use mnist data to train our CNN on. Our CNN is structured as following\n",
    "* a convolutional + relu layer\n",
    "* a max pool layer\n",
    "* a convolutional + relu layer\n",
    "* a max pool layer\n",
    "* one or two fully-connected layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\train-images-idx3-ubyte.gz\n",
      "Extracting .\\train-labels-idx1-ubyte.gz\n",
      "Extracting .\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('.', one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples 55000\n",
      "number of validation 5000\n",
      "input HxWxD = 28x28x1\n"
     ]
    }
   ],
   "source": [
    "print ('number of samples {}'.format(mnist.train.num_examples))\n",
    "print ('number of validation {}'.format(mnist.validation.num_examples))\n",
    "\n",
    "H = mnist.train.images[0].shape[0]\n",
    "W = mnist.train.images[0].shape[1]\n",
    "D = mnist.train.images[0].shape[2]\n",
    "\n",
    "print ('input HxWxD = {}x{}x{}'.format(H,W,D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 1st conv + relu + max-pool, output has shape HxWxD: 14x14x32\n",
      "after 1st conv + relu + max-pool, output has shape HxWxD: 7x7x64\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import tensorflow as tf\n",
    "\n",
    "# set up parameters\n",
    "learning_rate = 1.0e-5\n",
    "lr_adam = 1e-4\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# network parameters\n",
    "WC1 = 5     # width of 1st conv layer's weight\n",
    "HC1 = 5     # height of 1st conv layer's weight\n",
    "DC1 = 32    # depth of 1st conv layer's weight\n",
    "\n",
    "SC1 = 1     # stride for 1st conv layer\n",
    "PC1 = 'SAME' # padding for 1st conv layer to keep same H & W as input\n",
    "\n",
    "P1 = 2     # height & weight of pool window\n",
    "SP1 = 2     # stride for 1st max-pool layer\n",
    "OH1 = (H - P1) // SP1 + 1  \n",
    "OW1 = (W - P1) // SP1 + 1\n",
    "\n",
    "print ('after 1st conv + relu + max-pool, output has shape HxWxD: {}x{}x{}'.format(OH1, OW1, DC1) )\n",
    "WC2 = 5     # width of 2nd conv layer's weight\n",
    "HC2 = 5     # height of 2nd conv layer's weight\n",
    "DC2 = 64    # depth of 2nd conv layer's weight\n",
    "SC2 = 1     # stride for 2nd conv layer\n",
    "PC2 = 'SAME' # padding to keep same H & W as input\n",
    "\n",
    "P2 = 2\n",
    "SP2 = 2\n",
    "OH2 = (OH1 - P2) // SP2 + 1\n",
    "OW2 = (OW1 - P2) // SP2 + 1\n",
    "print ('after 1st conv + relu + max-pool, output has shape HxWxD: {}x{}x{}'.format(OH2, OW2, DC2) )\n",
    "\n",
    "FC1_in = OH2 * OW2 * DC2\n",
    "FC1_out = 1024\n",
    "\n",
    "n_classes = 10 # mnist total classes\n",
    "dropout = 0.5\n",
    "\n",
    "# validation parameters\n",
    "val_size = 256 # number of sample to compute the validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define/reset weights and biases\n",
    "Using above dimension input, we create weights/biases for our conv-layers & fc-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined wc1:0\n",
      "defined bc1:0\n",
      "defined wc2:0\n",
      "defined bc2:0\n",
      "defined wfc1:0\n",
      "defined bfc1:0\n",
      "defined wout:0\n",
      "defined bout:0\n"
     ]
    }
   ],
   "source": [
    "# reset to default, it will delete everything => all trained model will be lost\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# now we define some weight & biases\n",
    "wc1 = tf.Variable(tf.random_normal([WC1, HC1, D, DC1]), name = 'wc1')\n",
    "bc1 = tf.Variable(tf.random_normal([DC1]), name = 'bc1')\n",
    "\n",
    "wc2 = tf.Variable(tf.random_normal([WC2, HC2, DC1, DC2]), name = 'wc2')\n",
    "bc2 = tf.Variable(tf.random_normal([DC2]), name = 'bc2')\n",
    "\n",
    "wfc1 = tf.Variable(tf.random_normal([FC1_in, FC1_out]), name = 'wfc1')\n",
    "bfc1 = tf.Variable(tf.random_normal([FC1_out]), name = 'bfc1')\n",
    "\n",
    "wout = tf.Variable(tf.random_normal([FC1_out, n_classes]), name = 'wout')\n",
    "bout = tf.Variable(tf.random_normal([n_classes]), name = 'bout')\n",
    "\n",
    "for v in tf.global_variables():\n",
    "    print ('defined {}'.format(v.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers function\n",
    "We define some helper function to create conv-relu-layer/max-pool/affine-relu-dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, w, b, stride_conv, padding):\n",
    "    x = tf.nn.conv2d(x, w, strides = [1, stride_conv, stride_conv, 1], padding = padding)\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def max_pool2d(x, kernel_size, stride_pool):\n",
    "    return tf.nn.max_pool(x, \n",
    "                          ksize = [1, kernel_size, kernel_size, 1], \n",
    "                          strides = [1, stride_pool, stride_pool, 1], \n",
    "                          padding = 'SAME')\n",
    "\n",
    "def affine_relu_dropout(x, w, b, dropout):\n",
    "    x = tf.add(tf.matmul(x, w), b)\n",
    "    x = tf.nn.relu(x)\n",
    "    return tf.nn.dropout(x, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network construction\n",
    "We are ready to build our first conv-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, dropout):\n",
    "    # conv-relu-max_pool layer 1\n",
    "    conv1 = conv2d(x, wc1, bc1, SC1, PC1)\n",
    "    conv1 = max_pool2d(conv1, P1, SP1)\n",
    "    \n",
    "    # conv-relu-max_pool layer 2\n",
    "    conv2 = conv2d(conv1, wc2, bc2, SC2, PC2)\n",
    "    conv2 = max_pool2d(conv2, P2, SP2)\n",
    "    \n",
    "    # affine-relu-dropout layer 1\n",
    "    fc1 = tf.reshape(conv2, [-1, FC1_in])\n",
    "    fc1 = affine_relu_dropout(fc1, wfc1, bfc1, dropout)\n",
    "    \n",
    "    # output layer\n",
    "    out = tf.add(tf.matmul(fc1, wout), bout)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch   1 -Loss: 70585.6562 Validation Accuracy: 0.132812\n",
      "Epoch  1, Batch 101 -Loss: 16984.2012 Validation Accuracy: 0.367188\n",
      "Epoch  1, Batch 201 -Loss:  8803.8223 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch 301 -Loss:  8560.0605 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 401 -Loss:  5573.2241 Validation Accuracy: 0.769531\n",
      "Epoch  1 finished in   176.9627\n",
      "Epoch  2, Batch  72 -Loss:  2208.4587 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 172 -Loss:  1518.5900 Validation Accuracy: 0.812500\n",
      "Epoch  2, Batch 272 -Loss:  3001.0781 Validation Accuracy: 0.812500\n",
      "Epoch  2, Batch 372 -Loss:  1877.3879 Validation Accuracy: 0.828125\n",
      "Epoch  2 finished in   177.4979\n",
      "Epoch  3, Batch  43 -Loss:  1367.9121 Validation Accuracy: 0.847656\n",
      "Epoch  3, Batch 143 -Loss:  3036.6860 Validation Accuracy: 0.843750\n",
      "Epoch  3, Batch 243 -Loss:  1577.5485 Validation Accuracy: 0.843750\n",
      "Epoch  3, Batch 343 -Loss:  1482.0468 Validation Accuracy: 0.859375\n",
      "Epoch  3 finished in   182.3676\n",
      "Epoch  4, Batch  14 -Loss:   893.5799 Validation Accuracy: 0.851562\n",
      "Epoch  4, Batch 114 -Loss:  1530.2156 Validation Accuracy: 0.871094\n",
      "Epoch  4, Batch 214 -Loss:  1320.3176 Validation Accuracy: 0.867188\n",
      "Epoch  4, Batch 314 -Loss:  1887.9420 Validation Accuracy: 0.875000\n",
      "Epoch  4, Batch 414 -Loss:  1485.1002 Validation Accuracy: 0.878906\n",
      "Epoch  4 finished in   216.0811\n",
      "Epoch  5, Batch  85 -Loss:   873.1799 Validation Accuracy: 0.882812\n",
      "Epoch  5, Batch 185 -Loss:  1808.2085 Validation Accuracy: 0.875000\n",
      "Epoch  5, Batch 285 -Loss:   567.7490 Validation Accuracy: 0.886719\n",
      "Epoch  5, Batch 385 -Loss:   989.5309 Validation Accuracy: 0.894531\n",
      "Epoch  5 finished in   216.3282\n",
      "Epoch  6, Batch  56 -Loss:   783.4820 Validation Accuracy: 0.894531\n",
      "Epoch  6, Batch 156 -Loss:   358.4219 Validation Accuracy: 0.894531\n",
      "Epoch  6, Batch 256 -Loss:   586.7426 Validation Accuracy: 0.890625\n",
      "Epoch  6, Batch 356 -Loss:   804.3685 Validation Accuracy: 0.906250\n",
      "Epoch  6 finished in   217.0925\n",
      "Epoch  7, Batch  27 -Loss:   478.3394 Validation Accuracy: 0.894531\n",
      "Epoch  7, Batch 127 -Loss:   821.4984 Validation Accuracy: 0.906250\n",
      "Epoch  7, Batch 227 -Loss:   242.0187 Validation Accuracy: 0.910156\n",
      "Epoch  7, Batch 327 -Loss:   717.3251 Validation Accuracy: 0.906250\n",
      "Epoch  7, Batch 427 -Loss:  1263.6028 Validation Accuracy: 0.910156\n",
      "Epoch  7 finished in   207.5322\n",
      "Epoch  8, Batch  98 -Loss:   550.2327 Validation Accuracy: 0.914062\n",
      "Epoch  8, Batch 198 -Loss:   552.5210 Validation Accuracy: 0.921875\n",
      "Epoch  8, Batch 298 -Loss:   276.8212 Validation Accuracy: 0.917969\n",
      "Epoch  8, Batch 398 -Loss:  1082.9258 Validation Accuracy: 0.921875\n",
      "Epoch  8 finished in   179.2835\n",
      "Epoch  9, Batch  69 -Loss:   240.7938 Validation Accuracy: 0.914062\n",
      "Epoch  9, Batch 169 -Loss:   537.2522 Validation Accuracy: 0.914062\n",
      "Epoch  9, Batch 269 -Loss:   360.9524 Validation Accuracy: 0.921875\n",
      "Epoch  9, Batch 369 -Loss:   597.9501 Validation Accuracy: 0.917969\n",
      "Epoch  9 finished in   180.3697\n",
      "Epoch 10, Batch  40 -Loss:   623.9119 Validation Accuracy: 0.925781\n",
      "Epoch 10, Batch 140 -Loss:   258.9917 Validation Accuracy: 0.917969\n",
      "Epoch 10, Batch 240 -Loss:   280.0874 Validation Accuracy: 0.914062\n",
      "Epoch 10, Batch 340 -Loss:   320.5718 Validation Accuracy: 0.921875\n",
      "Epoch 10 finished in   178.6493\n",
      "Testing Accuracy: 0.9422000050544739\n"
     ]
    }
   ],
   "source": [
    "# Now let's try it\n",
    "import time\n",
    "x = tf.placeholder(tf.float32, [None, H, W, D], name = 'x')\n",
    "y = tf.placeholder(tf.float32, [None, n_classes], name = 'y')\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# create the conv-net\n",
    "logits =conv_net(x, keep_prob)\n",
    "\n",
    "# define the cost & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels = y))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr_adam).minimize(cost)\n",
    "\n",
    "# accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# init the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    irun = 0\n",
    "    skip_print = 100\n",
    "    for epoch in range(epochs):\n",
    "        ts = time.time()\n",
    "        for batch in range(mnist.train.num_examples // batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict = {x : batch_x, y : batch_y, keep_prob : dropout})\n",
    "            \n",
    "            # compute loss & accuracy\n",
    "            loss = sess.run(cost, feed_dict = {x : batch_x, y : batch_y, keep_prob : 1.})\n",
    "            val_acc = sess.run(accuracy, feed_dict = {x : mnist.validation.images[:val_size], \n",
    "                                                      y : mnist.validation.labels[:val_size], \n",
    "                                                      keep_prob : 1.})\n",
    "            \n",
    "            if irun % skip_print == 0:\n",
    "                print('Epoch {:>2}, Batch {:>3} -'\n",
    "                      'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(epoch + 1, batch + 1, loss, val_acc))\n",
    "            irun += 1\n",
    "        print('Epoch {:>2} finished in {:>10.4f}'.format(epoch + 1, time.time() - ts))\n",
    "    \n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                             y: mnist.test.labels,\n",
    "                                             keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}